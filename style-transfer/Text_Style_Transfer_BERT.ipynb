{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Style Transfer BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW3cB6bUtFod"
      },
      "source": [
        "# Text Style Transfer\n",
        "\n",
        "#####  Perform a rule-based text-style transfer by first training a token classifer to predict toxic tokens, then using antonyms to substitute such tokens.\n",
        "\n",
        "\n",
        "### Yakoob Khan '21\n",
        "### Date: March 18, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkSWiZpSmDro",
        "outputId": "20b2013f-a9df-4c0f-d23a-122fd63caf3b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CbaeH8tr-Sd",
        "outputId": "27577f19-7ccd-4955-c966-dd5a11823597"
      },
      "source": [
        "%cd \"drive/My Drive/toxic-comments-classification-challenge\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/toxic-comments-classification-challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IScCm-r3uIVY",
        "outputId": "79976ec2-9752-4c05-f96b-c72ab81dd935"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/d8/5144b0712f7f82229a8da5983a8fbb8d30cec5fbd5f8d12ffe1854dcea67/transformers-4.4.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 62.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=6f4fe8af639ff34143c4e4066763a61948399a87c66c54b1545937046c42c630\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V35JWAsusG24"
      },
      "source": [
        "### 1. Train a BERT Sequence Labeling Classifier to detect toxic tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EN3ICEksEqz"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import ast\n",
        "import string\n",
        "import itertools\n",
        "import argparse\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from ast import literal_eval\n",
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments, BertTokenizerFast "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DamgfujtBjc",
        "outputId": "dc28cf51-7cd3-4363-ce5a-df6d284ee1c8"
      },
      "source": [
        "def load_dataset(dataset_path):\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "    print(f\"\\n> Loading {dataset.shape[0]} examples located at '{dataset_path}'\\n\")\n",
        "\n",
        "    dataset[\"spans\"] = dataset.spans.apply(literal_eval)\n",
        "    texts, spans = dataset[\"text\"], dataset[\"spans\"]\n",
        "    texts = [text for text in texts]\n",
        "    spans = [span for span in spans]\n",
        "   \n",
        "    return texts, spans\n",
        "\n",
        "def load_asian_tweets_test_set(dataset_path):\n",
        "  dataset = pd.read_csv(dataset_path)\n",
        "  print(f\"\\n> Loading {dataset.shape[0]} test examples located at '{dataset_path}'\\n\")\n",
        "  texts = dataset[\"text\"]\n",
        "  texts = [text for text in texts]\n",
        "  categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "  labels = []\n",
        "  for i in range(len(texts)):\n",
        "    row_labels = [dataset[category][i] for category in categories]\n",
        "    labels.append(row_labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "def preserve_labels(text_encoding, span):\n",
        "  labels = [0] * len(text_encoding.tokens)\n",
        "  toxic_indices = set(span)\n",
        "  for i, offset in enumerate(text_encoding.offsets):\n",
        "    # labels for CLS, SEP and PAD tokens are set to -100.\n",
        "    if offset == (0, 0):\n",
        "      labels[i] = -100\n",
        "    \n",
        "    else:\n",
        "      # check if any character indices of this sub-token has gold label toxic \n",
        "      for k in range(offset[0], offset[1]):\n",
        "        if k in toxic_indices: \n",
        "          # toxic, so set label to 1.\n",
        "          labels[i] = 1\n",
        "          break\n",
        "  \n",
        "  return labels\n",
        "      \n",
        "def tokenize_data(tokenizer, texts, spans):\n",
        "    text_encodings = tokenizer(texts, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "    labels = [preserve_labels(text_encodings[i], span) for i, span in enumerate(spans)]\n",
        "    return text_encodings, labels\n",
        "\n",
        "\n",
        "def tokenize_testset(tokenizer, texts):\n",
        "    text_encodings = tokenizer(texts, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "    dummy_labels = [[0] * len(tokens) for i, tokens in enumerate(text_encodings.input_ids)]\n",
        "    return text_encodings, dummy_labels\n",
        "\n",
        "\n",
        "class ToxicSpansDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "import nltk\n",
        "\n",
        "# For tokenizing sentences\n",
        "nltk.download('punkt')\n",
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "\n",
        "\n",
        "def toxic_character_offsets_with_thresholding(post_num, tokens, offset_mapping, prediction, val_sentences_info, prediction_score, threshold):\n",
        "  toxic_offsets = []\n",
        "  scores = []\n",
        "  n = len(tokens)\n",
        "  i = 1           # start from 1 as 0th token is [CLS]\n",
        "  while i < n:\n",
        "    # stop looping after processing all post tokens\n",
        "    if tokens[i] == '[SEP]':\n",
        "      break\n",
        "\n",
        "    cur_toxic = []\n",
        "    # if previous token is also predicted toxic, then toxic phrase found\n",
        "    if len(toxic_offsets) > 0 and toxic_offsets[-1] == offset_mapping[i-1][1] - 1:\n",
        "      cur_toxic.extend([index for index in range(offset_mapping[i-1][1], offset_mapping[i][0])])\n",
        "    \n",
        "    # add the characters offsets of this head BPE\n",
        "    cur_toxic.extend([index for index in range(offset_mapping[i][0], offset_mapping[i][1])])\n",
        "    cur_score = [(tokens[i], prediction_score[i].max())]\n",
        "    cur_labels = [prediction[i]]\n",
        "    \n",
        "    # process all sub-tokens of the current head BPE\n",
        "    i += 1\n",
        "    while i < n and '##' in tokens[i]:\n",
        "      cur_toxic.extend([index for index in range(offset_mapping[i][0], offset_mapping[i][1])])\n",
        "      cur_score.append((tokens[i], prediction_score[i].max()))\n",
        "      cur_labels.append(prediction[i])\n",
        "      i += 1\n",
        "    \n",
        "    # word is predicted toxic if any sub-token is predicted toxic by model\n",
        "    prediction_label = True if max(cur_labels) == 1 else False\n",
        "    # prediction_label = True if min(cur_labels) == 1 else False\n",
        "\n",
        "    \n",
        "    # include cur_toxic offsets if any of the sub-token confidence score is greater than threshold\n",
        "    confidence_values = [score for _, score in cur_score]\n",
        "    passed_threshold = True if max(confidence_values) >= threshold else False\n",
        "    # passed_threshold = True if min(confidence_values) >= threshold else False\n",
        "\n",
        "    # include to global toxic offsets list only if both predicted label and threshold criteria passes\n",
        "    if prediction_label and passed_threshold:\n",
        "      toxic_offsets.extend(cur_toxic)\n",
        "      scores.extend(cur_score)\n",
        "  \n",
        "\n",
        "  return toxic_offsets, scores\n",
        "\n",
        "def character_offsets_with_thresholding(val_text_encodings, val_offset_mapping, predictions, val_sentences_info, prediction_scores, threshold=-float('inf')):\n",
        "  return [toxic_character_offsets_with_thresholding(i, val_text_encodings[i].tokens, offset_mapping, prediction, val_sentences_info, prediction_scores[i], threshold) for i, (offset_mapping, prediction) in enumerate(zip(val_offset_mapping, predictions))]\n",
        "\n",
        "def _contiguous_ranges(span_list):\n",
        "    \"\"\"Extracts continguous runs [1, 2, 3, 5, 6, 7] -> [(1,3), (5,7)].\n",
        "       Credit: https://github.com/ipavlopoulos/toxic_spans/blob/master/evaluation/fix_spans.py\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for _, span in itertools.groupby(\n",
        "        enumerate(span_list), lambda p: p[1] - p[0]):\n",
        "        span = list(span)\n",
        "        output.append((span[0][1], span[-1][1]))\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Db8Vibua80",
        "outputId": "663ad90b-11e2-41bb-e356-6da66204d2b0"
      },
      "source": [
        "# Load the BERT base cased tokenizer and pre-trained model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "\n",
        "# Load the train, val and test csv files\n",
        "training_texts, training_spans = load_dataset('./data/tsd_train.csv')\n",
        "val_texts, val_spans = load_dataset('./data/tsd_trial.csv')\n",
        "test_texts, labels  = load_asian_tweets_test_set('./data/asian_y_pred.csv')\n",
        "\n",
        "print(f\"> Total number of Scraped Tweets in Test Set : {len(test_texts)} \\n\")\n",
        "\n",
        "filtered_texts = []\n",
        "# Filter the test tweets that contain at least one toxicity predicted label\n",
        "for i in range(len(test_texts)):\n",
        "  if max(labels[i]) == 1:\n",
        "    filtered_texts.append(test_texts[i])\n",
        "\n",
        "test_texts = filtered_texts\n",
        "\n",
        "print(f\"> Filtered test set that contains at least one offensive language label prediction: {len(test_texts)} \\n\")\n",
        "\n",
        "val_sentences_info = {}\n",
        "print('\\n> Tokenizing text and generating word embeddings.. \\n')\n",
        "train_text_encodings, train_labels_encodings = tokenize_data(tokenizer, training_texts, training_spans)\n",
        "val_text_encodings, val_labels_encodings = tokenize_data(tokenizer, val_texts, val_spans)\n",
        "test_text_encodings, test_labels_encodings = tokenize_testset(tokenizer, test_texts)\n",
        "\n",
        "# Create Torch Dataset Objects for train / valid sets\n",
        "print('> Creating Tensor Datasets.. \\n')\n",
        "train_dataset = ToxicSpansDataset(train_text_encodings, train_labels_encodings)\n",
        "val_dataset = ToxicSpansDataset(val_text_encodings, val_labels_encodings)\n",
        "test_dataset = ToxicSpansDataset(test_text_encodings, test_labels_encodings)\n",
        "\n",
        "print(f\"> Training examples: {len(train_dataset)}\")\n",
        "print(f\"> Validation examples: {len(val_dataset)}\")\n",
        "print(f\"> Test examples: {len(test_dataset)}\\n\")\n",
        "\n",
        "# We don't want to pass offset mappings to the model\n",
        "train_offset_mapping = train_text_encodings.pop(\"offset_mapping\") \n",
        "val_offset_mapping = val_text_encodings.pop(\"offset_mapping\")\n",
        "test_offset_mapping = test_text_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "# Training Argument Object with hyper-parameter configuration.\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./logs',                    # output directory\n",
        "  num_train_epochs=2,                     # total number of training epochs\n",
        "  per_device_train_batch_size=8,         # batch size per device during training\n",
        "  per_device_eval_batch_size=8,          # batch size for evaluation\n",
        "  warmup_steps=500,                       # number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01,                      # strength of weight decay\n",
        "  logging_dir='./logs',                   # directory for storing logs\n",
        "  logging_steps=100,                      # log after every x steps\n",
        "  do_eval=True,                           # whether to run evaluation on the val set\n",
        "  evaluation_strategy=\"steps\",            # evaluation is done (and logged) every logging_steps \n",
        "  learning_rate=5e-5,                     # 5e-5 is default learning rate\n",
        "  disable_tqdm=True,                      # remove tqdm statements to reduce clutter\n",
        ")\n",
        "\n",
        "# Trainer Object\n",
        "trainer = Trainer(\n",
        "  model=model,                 # the instantiated ðŸ¤— Transformers model to be trained\n",
        "  args=training_args,          # training arguments, defined above\n",
        "  train_dataset=train_dataset,       \n",
        "  eval_dataset=val_dataset,         \n",
        ")\n",
        "\n",
        "print('> Started Toxic Spans Detection training! \\n')\n",
        "trainer.train()\n",
        "\n",
        "print('> Making toxic token predictions on Scraped Asian Tweets \\n')\n",
        "# use trained model to make toxic token predictions on test datasets\n",
        "test_pred = trainer.predict(test_dataset)\n",
        "\n",
        "# retrieve the predictions\n",
        "test_predictions = test_pred.predictions.argmax(-1)\n",
        "test_prediction_scores = test_pred.predictions\n",
        "test_toxic_char_preds = character_offsets_with_thresholding(test_text_encodings, test_offset_mapping, test_predictions, val_sentences_info,  test_prediction_scores, threshold=-float('inf'))\n",
        "\n",
        "toxic_char_offsets = [span[0] for span in test_toxic_char_preds]\n",
        "test_set_toxic_tokens = {'text': [], 'spans': []}\n",
        "\n",
        "for text, pred in zip(test_texts, toxic_char_offsets):\n",
        "  test_set_toxic_tokens['text'].append(text)\n",
        "  test_set_toxic_tokens['spans'].append(pred)\n",
        "\n",
        "# Save the toxic span predictions in a CSV file\n",
        "df = pd.DataFrame(test_set_toxic_tokens)\n",
        "df.to_csv('./style-transfer/asian_tweet_toxic_token_predictions.csv', index=False)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Loading 7939 examples located at './data/tsd_train.csv'\n",
            "\n",
            "\n",
            "> Loading 690 examples located at './data/tsd_trial.csv'\n",
            "\n",
            "\n",
            "> Loading 28408 test examples located at './data/asian_y_pred.csv'\n",
            "\n",
            "> Total number of Scraped Tweets in Test Set : 28408 \n",
            "\n",
            "> Filtered test set that contains at least one offensive language label prediction: 2288 \n",
            "\n",
            "\n",
            "> Tokenizing text and generating word embeddings.. \n",
            "\n",
            "> Creating Tensor Datasets.. \n",
            "\n",
            "> Training examples: 7939\n",
            "> Validation examples: 690\n",
            "> Test examples: 2288\n",
            "\n",
            "> Started Toxic Spans Detection training! \n",
            "\n",
            "{'loss': 0.4037, 'learning_rate': 1e-05, 'epoch': 0.1}\n",
            "{'eval_loss': 0.22229667007923126, 'eval_runtime': 3.3458, 'eval_samples_per_second': 206.229, 'epoch': 0.1}\n",
            "{'loss': 0.2761, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
            "{'eval_loss': 0.20020772516727448, 'eval_runtime': 3.3487, 'eval_samples_per_second': 206.05, 'epoch': 0.2}\n",
            "{'loss': 0.2494, 'learning_rate': 3e-05, 'epoch': 0.3}\n",
            "{'eval_loss': 0.20238102972507477, 'eval_runtime': 3.3444, 'eval_samples_per_second': 206.318, 'epoch': 0.3}\n",
            "{'loss': 0.2301, 'learning_rate': 4e-05, 'epoch': 0.4}\n",
            "{'eval_loss': 0.19794273376464844, 'eval_runtime': 3.3465, 'eval_samples_per_second': 206.183, 'epoch': 0.4}\n",
            "{'loss': 0.2315, 'learning_rate': 5e-05, 'epoch': 0.5}\n",
            "{'eval_loss': 0.20240190625190735, 'eval_runtime': 3.3616, 'eval_samples_per_second': 205.257, 'epoch': 0.5}\n",
            "{'loss': 0.2561, 'learning_rate': 4.663526244952894e-05, 'epoch': 0.6}\n",
            "{'eval_loss': 0.1903972625732422, 'eval_runtime': 3.3392, 'eval_samples_per_second': 206.633, 'epoch': 0.6}\n",
            "{'loss': 0.2518, 'learning_rate': 4.327052489905787e-05, 'epoch': 0.7}\n",
            "{'eval_loss': 0.19595454633235931, 'eval_runtime': 3.361, 'eval_samples_per_second': 205.294, 'epoch': 0.7}\n",
            "{'loss': 0.1937, 'learning_rate': 3.990578734858681e-05, 'epoch': 0.81}\n",
            "{'eval_loss': 0.18396569788455963, 'eval_runtime': 3.3407, 'eval_samples_per_second': 206.546, 'epoch': 0.81}\n",
            "{'loss': 0.2421, 'learning_rate': 3.654104979811575e-05, 'epoch': 0.91}\n",
            "{'eval_loss': 0.18223947286605835, 'eval_runtime': 3.351, 'eval_samples_per_second': 205.907, 'epoch': 0.91}\n",
            "{'loss': 0.2305, 'learning_rate': 3.317631224764469e-05, 'epoch': 1.01}\n",
            "{'eval_loss': 0.1871003359556198, 'eval_runtime': 3.3544, 'eval_samples_per_second': 205.699, 'epoch': 1.01}\n",
            "{'loss': 0.2338, 'learning_rate': 2.981157469717362e-05, 'epoch': 1.11}\n",
            "{'eval_loss': 0.19457533955574036, 'eval_runtime': 3.342, 'eval_samples_per_second': 206.463, 'epoch': 1.11}\n",
            "{'loss': 0.2338, 'learning_rate': 2.644683714670256e-05, 'epoch': 1.21}\n",
            "{'eval_loss': 0.19046038389205933, 'eval_runtime': 3.3485, 'eval_samples_per_second': 206.063, 'epoch': 1.21}\n",
            "{'loss': 0.1889, 'learning_rate': 2.3082099596231497e-05, 'epoch': 1.31}\n",
            "{'eval_loss': 0.19398905336856842, 'eval_runtime': 3.3609, 'eval_samples_per_second': 205.304, 'epoch': 1.31}\n",
            "{'loss': 0.2089, 'learning_rate': 1.971736204576043e-05, 'epoch': 1.41}\n",
            "{'eval_loss': 0.19338463246822357, 'eval_runtime': 3.3643, 'eval_samples_per_second': 205.095, 'epoch': 1.41}\n",
            "{'loss': 0.1975, 'learning_rate': 1.635262449528937e-05, 'epoch': 1.51}\n",
            "{'eval_loss': 0.1917904019355774, 'eval_runtime': 3.3611, 'eval_samples_per_second': 205.292, 'epoch': 1.51}\n",
            "{'loss': 0.1758, 'learning_rate': 1.2987886944818307e-05, 'epoch': 1.61}\n",
            "{'eval_loss': 0.19473563134670258, 'eval_runtime': 3.3585, 'eval_samples_per_second': 205.448, 'epoch': 1.61}\n",
            "{'loss': 0.1888, 'learning_rate': 9.623149394347242e-06, 'epoch': 1.71}\n",
            "{'eval_loss': 0.19871193170547485, 'eval_runtime': 3.3543, 'eval_samples_per_second': 205.704, 'epoch': 1.71}\n",
            "{'loss': 0.1905, 'learning_rate': 6.258411843876178e-06, 'epoch': 1.81}\n",
            "{'eval_loss': 0.19562776386737823, 'eval_runtime': 3.3608, 'eval_samples_per_second': 205.308, 'epoch': 1.81}\n",
            "{'loss': 0.157, 'learning_rate': 2.8936742934051144e-06, 'epoch': 1.91}\n",
            "{'eval_loss': 0.20149478316307068, 'eval_runtime': 3.3574, 'eval_samples_per_second': 205.518, 'epoch': 1.91}\n",
            "{'train_runtime': 643.7819, 'train_samples_per_second': 3.085, 'epoch': 2.0}\n",
            "> Making toxic token predictions on Scraped Asian Tweets \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FaIWZApSCT7j",
        "outputId": "8dbbbddb-b9ee-46f3-ee6b-793177a9e522"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "def neutralize(text, span):\n",
        "  toxic_offsets = set(span)\n",
        "  # tokenize the text using NLTK\n",
        "  try:\n",
        "    token_indices = twt().span_tokenize(text)\n",
        "    # loop through the tokens and convert any toxic tokens to antonym\n",
        "    neutral = ''\n",
        "    for i, j in token_indices:\n",
        "      toxic = False\n",
        "      for k in range(i, j+1):\n",
        "        if k in toxic_offsets:\n",
        "          toxic = True\n",
        "          break\n",
        "      \n",
        "      token = text[i:j+1]\n",
        "      if not toxic:\n",
        "        neutral += f\"{token} \"\n",
        "      else:\n",
        "        antonyms = []\n",
        "        # find all the antonyms of this word\n",
        "        # Credit: https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/\n",
        "        for syn in wordnet.synsets(token): \n",
        "          for l in syn.lemmas(): \n",
        "            if l.antonyms(): \n",
        "                antonyms.append(l.antonyms()[0].name()) \n",
        "\n",
        "        # pick a random antonym if there is any\n",
        "        \n",
        "        substituted_token = random.choice(antonyms) if antonyms else \"***\"\n",
        "        neutral += f\"{substituted_token} \"\n",
        "    \n",
        "    return neutral.strip()\n",
        "\n",
        "  except:\n",
        "    # twt().span_tokenize(text) raises error sometimes, so catch all statement to return original text\n",
        "    return text\n",
        "\n",
        "asian_test_texts, test_spans = load_dataset('./style-transfer/asian_tweet_toxic_token_predictions.csv')\n",
        "neutralized_dict = {'original': [], 'neutral': []}\n",
        "\n",
        "for text, span in zip(asian_test_texts, test_spans):\n",
        "  \n",
        "  neutralized_dict['original'].append(text)\n",
        "  neutralized_text = neutralize(text, span)\n",
        "  neutralized_dict['neutral'].append(neutralized_text)\n",
        "\n",
        "# Save the neutralized posts in a CSV file\n",
        "df = pd.DataFrame(neutralized_dict)\n",
        "df.to_csv('./style-transfer/asian_toxic_tweet_neutralized.csv', index=False)\n",
        "df"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "> Loading 2288 examples located at './style-transfer/asian_tweet_toxic_token_predictions.csv'\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>These morons attack an Asian business owner fo...</td>\n",
              "      <td>These  *** attack  an  Asian  business  owner ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Last tweet of the night... in 2012, I truley t...</td>\n",
              "      <td>Last  tweet  of  the  night. ...  in  2012, , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@THR Could a closeted gay asian man be a chink...</td>\n",
              "      <td>@T THR  Could  a  closeted  *** asian  man  be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When youâ€™re Muslim, and only want/need one wif...</td>\n",
              "      <td>When  youâ€™ â€™r re  Muslim, ,  and  only  want/n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@JGreenblattADL Is all trumps fault. His vicio...</td>\n",
              "      <td>@J JGreenblattADL  Is  all  trumps  fault.  Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2283</th>\n",
              "      <td>Be afraid, be very afraid taxpayers! Thru the ...</td>\n",
              "      <td>Be  afraid, ,  be  very  afraid  taxpayers! ! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284</th>\n",
              "      <td>It shouldnâ€™t surprise anyone that the previous...</td>\n",
              "      <td>It  shouldnâ€™ â€™t t  surprise  anyone  that  the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2285</th>\n",
              "      <td>@guardian Meghan is country girl turned duches...</td>\n",
              "      <td>@g guardian  Meghan  is  country  girl  turned...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2286</th>\n",
              "      <td>The source article, with warnings from Chines ...</td>\n",
              "      <td>The  source  article, ,  with  warnings  from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>@oldtownhooper15 @espn @JLin7 you literally ge...</td>\n",
              "      <td>@o oldtownhooper15  @e espn  @J JLin7  you  li...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2288 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               original                                            neutral\n",
              "0     These morons attack an Asian business owner fo...  These  *** attack  an  Asian  business  owner ...\n",
              "1     Last tweet of the night... in 2012, I truley t...  Last  tweet  of  the  night. ...  in  2012, , ...\n",
              "2     @THR Could a closeted gay asian man be a chink...  @T THR  Could  a  closeted  *** asian  man  be...\n",
              "3     When youâ€™re Muslim, and only want/need one wif...  When  youâ€™ â€™r re  Muslim, ,  and  only  want/n...\n",
              "4     @JGreenblattADL Is all trumps fault. His vicio...  @J JGreenblattADL  Is  all  trumps  fault.  Hi...\n",
              "...                                                 ...                                                ...\n",
              "2283  Be afraid, be very afraid taxpayers! Thru the ...  Be  afraid, ,  be  very  afraid  taxpayers! ! ...\n",
              "2284  It shouldnâ€™t surprise anyone that the previous...  It  shouldnâ€™ â€™t t  surprise  anyone  that  the...\n",
              "2285  @guardian Meghan is country girl turned duches...  @g guardian  Meghan  is  country  girl  turned...\n",
              "2286  The source article, with warnings from Chines ...  The  source  article, ,  with  warnings  from ...\n",
              "2287  @oldtownhooper15 @espn @JLin7 you literally ge...  @o oldtownhooper15  @e espn  @J JLin7  you  li...\n",
              "\n",
              "[2288 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}