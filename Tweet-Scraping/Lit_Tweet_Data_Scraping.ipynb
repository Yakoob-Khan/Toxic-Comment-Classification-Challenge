{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction for Anti-Asian Biased tweets using Twitter Search API\n",
    "\n",
    "## Author: Luca Chun Lun Lit\n",
    "### Date: Aug 22 2020\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages to connect to Twitter's API + miscellaneous data tools\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tweepy Authentication + API connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# authentication information (obtained from Twitter Developer Lab)\n",
    "consumer_key = \"wvrISJ5kF3i40Ul98xcDvxGEF\"\n",
    "consumer_secret = \"11ba9Mdzx7RyLHQIV5me3d9zZ2g3fF8PsGc6shR52F0crXlbby\"\n",
    "access_token = \"1273333809510055936-PD9haFugtPNoqtnk3GLhPsq0UmkqVy\"\n",
    "access_secret = \"B4sl4xHWP4fsduOC2d9OIrDbDj7JGzt0WptkAuepENtmp\"\n",
    "\n",
    "\n",
    "# creating an OAuthHandler instance and passing it the auth. info.\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# connecting to Twitter API\n",
    "api = tweepy.API(auth, wait_on_rate_limit= True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "\n",
    "except:\n",
    "    print(\"Error during authentication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimized Algorithm to exploit rate limits by saving tweets intermittently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that saves tweet objects and its attributes to a pandas dataframe\n",
    "# Input: a pandas dataframe and a list of tweet objects\n",
    "def save_list_to_pandas(dataf, list):\n",
    "    print(\"\\nExtracting and saving tweet attributes to pandas dataframe ...\")\n",
    "\n",
    "    # TWEET ATTRIBUTES:\n",
    "\n",
    "    # tweet_id - unique ID of the tweet\n",
    "    # user.verified - boolean of whether the user is verified or not\n",
    "    # user.time_zone - time_zone of user\n",
    "    # user.geo_enabled - whether the user enabled geo-tracking on account\n",
    "    # favorite_count - number of likes the tweet received\n",
    "    # user.screen_name - username of the tweeter\n",
    "    # user.location - geolocational data of where the tweet was posted - requires users to enable the location option\n",
    "    # user.friends_count - number of people the user follows\n",
    "    # user.followers_count - number of people following the user\n",
    "    # user.statuses_count - total tweets by user\n",
    "    # user.created_at - when the user account was created\n",
    "    # tweet_url - URL to the tweet\n",
    "    # created_at - tweet creation date and time\n",
    "    # retweet_count - number of retweets\n",
    "    # retweeted_status.full_text - full text of the tweet\n",
    "    # entities['hashtags'] - hashtags in the tweet\n",
    "    # entitities['urls'] - urls embedded in the tweet\n",
    "\n",
    "    # Looping through the input list and extracting the attributes from each tweet\n",
    "    for tweet in list:\n",
    "\n",
    "        tweet_id = tweet.id_str\n",
    "        verified = tweet.user.verified\n",
    "        source = tweet.source\n",
    "        in_reply_to_status_id = tweet.in_reply_to_status_id\n",
    "        in_reply_to_status_id_str = tweet.in_reply_to_status_id_str\n",
    "        in_reply_to_user_id = tweet.in_reply_to_user_id\n",
    "        in_reply_to_user_id_str = tweet.in_reply_to_user_id_str\n",
    "        in_reply_to_screen_name = tweet.in_reply_to_screen_name\n",
    "        screen_name = tweet.user.name\n",
    "        description = tweet.user.description\n",
    "        listed_count = tweet.user.listed_count\n",
    "        utc_offset = tweet.user.utc_offset\n",
    "        lang = tweet.user.lang\n",
    "        contributers_enabled = tweet.user.contributors_enabled\n",
    "        is_translator = tweet.user.is_translator\n",
    "        profile_bg_img_url = tweet.user.profile_background_image_url\n",
    "        profile_bg_img_url_https = tweet.user.profile_background_image_url_https\n",
    "        profile_img_url = tweet.user.profile_image_url\n",
    "        profile_img_url_https = tweet.user.profile_image_url_https\n",
    "        default_profile = tweet.user.default_profile\n",
    "        default_profile_img = tweet.user.default_profile_image\n",
    "\n",
    "        time_zone = tweet.user.time_zone\n",
    "        geo_enabled = tweet.user.geo_enabled\n",
    "        favorite_count = tweet.favorite_count\n",
    "        user_id = tweet.user.id_str\n",
    "        username = tweet.user.screen_name\n",
    "        location = tweet.user.location\n",
    "        following = tweet.user.friends_count\n",
    "        followers = tweet.user.followers_count\n",
    "        totaltweets = tweet.user.statuses_count\n",
    "        tweet_url = \"https://twitter.com/\" + username + \"/status/\" + tweet_id\n",
    "        tweetcreatedts = tweet.created_at\n",
    "        retweetcount = tweet.retweet_count\n",
    "        hashtags = tweet.entities['hashtags']\n",
    "        mentions = tweet.entities['user_mentions']\n",
    "        urls = tweet.entities['urls']\n",
    "\n",
    "        try:  # If the tweet is a retweet\n",
    "            text = tweet.retweeted_status.full_text\n",
    "            retweet_status = True\n",
    "        except AttributeError:  # If the tweet is not a retweet\n",
    "            text = tweet.full_text\n",
    "            retweet_status = False\n",
    "\n",
    "        # create a list storing tweet attributes in the following form (indices corresponding to the pandas dataframe above)\n",
    "        attr = [user_id, screen_name, username, verified, description, location, geo_enabled, time_zone,\n",
    "                following, followers, totaltweets, tweetcreatedts, tweet_url,\n",
    "                tweet_id, text, retweetcount, retweet_status, favorite_count,\n",
    "                hashtags, mentions, urls, source, in_reply_to_status_id, in_reply_to_status_id_str,\n",
    "                               in_reply_to_user_id, in_reply_to_user_id_str, in_reply_to_screen_name, listed_count, utc_offset, lang, contributers_enabled, is_translator, profile_bg_img_url, profile_bg_img_url_https, profile_img_url, profile_img_url_https, default_profile, default_profile_img]\n",
    "\n",
    "        # add this tweet's attr to the pandas dataframe\n",
    "        dataf.loc[len(dataf)] = attr\n",
    "\n",
    "    # Notify user of how many new tweets have been saved to pandas dataframe, and how many tweets now exist in the dataframe\n",
    "    print(\"Saved \" + str(len(list)) + \" tweets to pandas dataframe\")\n",
    "    print(\"Dataframe now contains \" + str(len(dataf)) + \" tweets\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to check compute how many days have elapsed from today to a user-provided date.\n",
    "# Input: a date in the format YYYY-MM-DD\n",
    "# Output: The number of days elapsed\n",
    "def valid_date_check(date_since, date_until):\n",
    "\n",
    "    # Get today's date and format it to YYYY, MM, DD\n",
    "    today = date.today().strftime('%Y-%m-%d')\n",
    "    today_year = int(today[:4]); today_month = int(today[5:7]); today_day = int(today[8:])\n",
    "\n",
    "    # Get the date_since and format it to YYYY, MM, DD\n",
    "    since_year = int(date_since[:4]); since_month = int(date_since[5:7]); since_day = int(date_since[8:])\n",
    "    d0 = date(today_year, today_month, today_day)\n",
    "    d1 = date(since_year, since_month, since_day)\n",
    "\n",
    "    # compute how many days ave elapsed between since date and today\n",
    "    since_today_dayspan = d0 - d1\n",
    "\n",
    "    until_year = int(date_until[:4]); until_month = int(date_until[5:7]); until_day = int(date_until[8:])\n",
    "    d2 = date(until_year, until_month, until_day)\n",
    "    since_until_dayspan = d2 - d1\n",
    "\n",
    "    return since_today_dayspan.days, since_today_dayspan.days\n",
    "\n",
    "\n",
    "\n",
    "# Function to scrape tweets based on a keyword, since and until date, and optional number of Tweets to scrape\n",
    "# Input: (str) query words, (YYYY-MM-DD) date to be searched from, (YYYY-MM-DD) date to be searched until\n",
    "def scrapetweets(search_words, date_since, date_until = \"\"):\n",
    "\n",
    "    if date_until == \"\":\n",
    "        date_until = (datetime.strptime(date_since, '%Y-%m-%d') + timedelta(1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    # If entered date_since is more than 7 days ago, Twitter is unable to retrieve the tweets\n",
    "    # Below is a check to make sure a valid since date is entered\n",
    "    days_elapsed = valid_date_check(date_since, date_until)\n",
    "\n",
    "    # If greater than 7, then print error message and return\n",
    "    if days_elapsed[0] > 8:\n",
    "        print(\"\\nTwitter API only allows searching for tweets published in the past 7 days. Please enter a valid date and re-run.\")\n",
    "        return\n",
    "    elif days_elapsed[0] < 0:\n",
    "        print(\"\\nThe 'Since Date' field entered is a time in the future. Please enter a valid date and re-run.\")\n",
    "        return\n",
    "\n",
    "    if days_elapsed[1] <= 0:\n",
    "        print(\"\\nInvalid 'Until Date' field entered. Date provided is either 1) before 'Since Date' or 2) same as 'Since Date'. Please enter a valid date and re-run.\")\n",
    "        return\n",
    "    # record start_time of program\n",
    "    scrap_start = time.time()\n",
    "    print(\"\\nProgram start time: \" + (datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "    # Timestamp current date and time\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "    # Define a pandas dataframe to store the data, with the following features:\n",
    "    df = pd.DataFrame(columns=['user_id', 'screen_name','username', 'verified', 'description', 'location', 'geo_enabled', 'time_zone',\n",
    "                     'following', 'followers', 'totaltweets', 'tweetcreatedts', 'tweet_url',\n",
    "                     'tweet_id', 'text', 'retweetcount', 'retweet_status', 'favorite_count',\n",
    "                     'hashtags', 'mentions','urls', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "                               'in_reply_to_user_id', 'in_reply_to_user_str', 'in_reply_to_screen_name', 'listed_count', 'utc_offset', 'lang', 'contributers_enabled', 'is_translator', 'profile_bg_img_url', 'profile_bg_img_url_https', 'profile_img_url', 'profile_img_url_https', 'default_profile', 'default_profile_img']\n",
    "                            )\n",
    "\n",
    "    print(\"\\nRunning Search on:\", search_words)\n",
    "    print(\"Since: {} 00:00:00, Until {} 11:59:59\".format(date_since, date_since))\n",
    "\n",
    "    print(\"\\nExtracting Cursor Object ...\")\n",
    "\n",
    "    # We will use the Tweepy cursor object to extract Twitter data in the form of cursor objects, which we can iterate through\n",
    "    # Each item that is iterated over has common attributes that we can access (i.e. user_id, tweet_id, tweet_body)\n",
    "    # Set each call to the API to retrieve 100 tweets, and return tweets in extended format.\n",
    "    tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", count = 100, since=date_since, until=date_until,tweet_mode='extended').items(2000)\n",
    "\n",
    "    print(\"Obtained Cursor Object :\", tweets)\n",
    "    print(\"\\nIterating over Tweet Objects in Cursor Object ...\")\n",
    "    print(\"Press 'Command + F2' at anytime to stop converting Tweet Objects to list\")\n",
    "\n",
    "    # Convert the tweet objects into list form\n",
    "    tweet_list = []\n",
    "    count = 0\n",
    "    try:\n",
    "        lim = api.rate_limit_status()[\"resources\"][\"search\"][\"/search/tweets\"][\"remaining\"]\n",
    "        if lim == 0:\n",
    "            time.sleep(900)\n",
    "            print(\"Rate Limit reached. Waiting 15 minutes to replenish rates.\")\n",
    "        for tweet in tweets:\n",
    "\n",
    "            # if the rate limit is reached\n",
    "            if lim < 2:\n",
    "                # we know the rate limit must be reached --> therefore, we can begin saving the list to pandas dataframe while we wait for the rate to replenish\n",
    "                print(\"Rate limit reached. Time to wait: 900 seconds\")\n",
    "                t1 = time.time()\n",
    "                print(\"Saving extracted tweet data to pandas dataframe in the meanwhile ...\")\n",
    "                save_list_to_pandas(df, tweet_list)\n",
    "                print(\"Latest time recorded in dataframe: \", df[\"tweetcreatedts\"].iloc[-1])\n",
    "                t2 = time.time()\n",
    "                if t2 - t1 < 900:  # see how long saving takes - if it is less than 15 minutes, wait until the 15 minute mark\n",
    "                    time.sleep(math.ceil(900 - (t2 - t1)))\n",
    "                tweet_list = []\n",
    "                time.sleep(5)\n",
    "                lim = api.rate_limit_status()[\"resources\"][\"search\"][\"/search/tweets\"][\"remaining\"]\n",
    "\n",
    "            tweet_list.append(tweet)\n",
    "            count += 1\n",
    "            if count % 3000 == 0:\n",
    "                print(\"Checkpoint: converted \" + str(count) + \" tweets\")\n",
    "            if count % 100 == 0:\n",
    "                lim -= 1\n",
    "\n",
    "        # if there are no more tweets to be searched, then save the rest of the list to pandas dataframe\n",
    "        print(\"Last set of tweet data reached.\")\n",
    "        save_list_to_pandas(df, tweet_list)\n",
    "\n",
    "    except KeyboardInterrupt:                               # to prevent loss of data, ctrl + C can be pressed to stop converting tweets and move on to saving data\n",
    "        save_list_to_pandas(df, tweet_list)\n",
    "        pass\n",
    "\n",
    "    print('\\nTweet scraping and saving to pandas dataframe complete. Total number of Tweets scraped is: {}'.format(count))\n",
    "    print(\"Total Number of API requests made: \", math.ceil(count / 100))\n",
    "\n",
    "    # Find the current directory path, and construct a new file under the 'data' folder\n",
    "    curr_path = os.getcwd()\n",
    "    filepath = curr_path + 'Anti-Asian_tweets.csv'\n",
    "\n",
    "    # check if file already exists, and provide the user with the option to overwrite the file - or not\n",
    "    if os.path.isfile(filepath):\n",
    "        over_write = input('This file already exists. Would you like to overwrite it? Type Y = yes, N = no\\n')\n",
    "        if over_write.lower() == 'y':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Results were not saved - no data overwritten.\")\n",
    "            return\n",
    "\n",
    "    print('\\nSaving data in .CSV format now ...')\n",
    "    # Convert pandas dataframe to csv format, using the file path created above\n",
    "    df.to_csv(filepath, index=False, float_format='%.13f')\n",
    "\n",
    "    scrap_end = time.time()\n",
    "    print('Complete! Saved as ' + date_since + '_tweets.csv. Total run time is {} minutes.'.format(round(scrap_end - scrap_start) / 60, 2))\n",
    "\n",
    "    print(\"\\nProgram end time: \" + (datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program start time: 2021-03-11 03:35:31\n",
      "\n",
      "Running Search on: (\"china virus\" OR \"kung flu\" OR \"kungflu\" OR \"Wuhan\" OR \"plague\" OR \"chinese virus\" OR \"Wuhan Flu\" OR \"ching chong\" OR \"chink\" OR \"chinavirus\" OR \"chinesevirus\") -filter:retweets\n",
      "Since: 2021-03-03 00:00:00, Until 2021-03-03 11:59:59\n",
      "\n",
      "Extracting Cursor Object ...\n",
      "Obtained Cursor Object : <tweepy.cursor.ItemIterator object at 0x11763c630>\n",
      "\n",
      "Iterating over Tweet Objects in Cursor Object ...\n",
      "Press 'Command + F2' at anytime to stop converting Tweet Objects to list\n",
      "Last set of tweet data reached.\n",
      "\n",
      "Extracting and saving tweet attributes to pandas dataframe ...\n",
      "Saved 2000 tweets to pandas dataframe\n",
      "Dataframe now contains 2000 tweets\n",
      "\n",
      "Tweet scraping and saving to pandas dataframe complete. Total number of Tweets scraped is: 2000\n",
      "Total Number of API requests made:  20\n",
      "This file already exists. Would you like to overwrite it? Type Y = yes, N = no\n",
      "Y\n",
      "\n",
      "Saving data in .CSV format now ...\n",
      "Complete! Saved as 2021-03-03_tweets.csv. Total run time is 0.6833333333333333 minutes.\n",
      "\n",
      "Program end time: 2021-03-11 03:36:12\n"
     ]
    }
   ],
   "source": [
    "# Running the search with user-preference inputs\n",
    "#search_words = '(hospital OR health OR medical OR medicine OR cure) (COVID-19 OR coronavirus OR covid OR nCov OR ncov19 OR 2019ncov OR corona)'\n",
    "#search_words = '(health OR medical OR medicine OR cure) (COVID-19 OR coronavirus OR covid OR corona)'\n",
    "#search_words = 'tele-health OR telehealth OR \"tele health\" OR televisit OR tele-visit OR \"tele visit\" OR \"virtual healthcare\" OR \"virtual health care\" OR \"virtual care\" OR virtualcare OR virtualhealthcare OR tele-medicine OR telemedicine OR \"tele medicine\" OR telemed OR TelemedNow OR digitalhealth OR \"digital health\" OR digihealth OR \"remote monitoring\" OR remotemonitoring OR connectedhealth OR \"connected health\"'\n",
    "\n",
    "search_words = '(\"china virus\" OR \"kung flu\" OR \"kungflu\" OR \"Wuhan\" OR \"plague\" OR \"chinese virus\" OR \"Wuhan Flu\" OR \"ching chong\" OR \"chink\" OR \"chinavirus\" OR \"chinesevirus\") -filter:retweets'\n",
    "\n",
    "#  \"plague\" OR \"disgusting\" OR \"dirty\" OR \"gross\" OR \"immigrant\" OR \"steal\" OR \"stealing\" \"go home\" OR \"karma\"\n",
    "\n",
    "date_since = \"2021-03-03\"\n",
    "date_until = \"2021-03-10\"\n",
    "\n",
    "# Call the function scraptweets\n",
    "data = scrapetweets(search_words, date_since, date_until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       @NovayaIra @kayleighmcenany @FoxBusiness @Davi...\n",
       "1       I just backed The Plague and Doctor Caim on @K...\n",
       "2       @dbrown13 I don't know about that. The same de...\n",
       "3       @TerryToohey The Bubonic Plague took a real to...\n",
       "4       O malevolent gxds, deliver me from this plague...\n",
       "5       Excellent piece for those most affected by our...\n",
       "6       @Robcass78 @RogerGalligan @StuartDNeilson Does...\n",
       "7       people runnnn from accountability like it’s th...\n",
       "8                     Ching Chong https://t.co/yIy2ksqL2V\n",
       "9       @abo_fawwaz1394 @TrumanDoctrine_ @AsaadHannaa ...\n",
       "10      @Williani4 //Have a Dr. Plague, he's 14'5\" htt...\n",
       "11      @ksooruns And u stan ching chong sound fave🤡🐵 ...\n",
       "12                          @Smol_And_Busty Can you dm me\n",
       "13      It's the same inequality that was the content ...\n",
       "14      How about speaking out against a certain forme...\n",
       "15      @MenInKiltsSTARZ @SamHeughan Plague doctor; if...\n",
       "16       nightmare blunt rotation https://t.co/8dxRScdQmj\n",
       "17      @beard1000 🤔 Their anti-plague nasal spray cou...\n",
       "18      @opaldenska @DavidRobertGor1 @EstaDePiPi @crus...\n",
       "19      @DineshDSouza why sir has the gain of function...\n",
       "20      @sunlorrie Today is a good day to revisit @The...\n",
       "21      @larryelder They’ll need it... I’d avoid that ...\n",
       "22                         @_mick_plague_ ur my idol so x\n",
       "23      @ShannonJoyRadio The Black Plague lasted from ...\n",
       "24      My mother: “isn’t captain marvel captain Ameri...\n",
       "25      @imsuchakilljoy artcrypto/btc miners 1000 year...\n",
       "26                            @KnifeDetective plague mask\n",
       "27      Forgeworld is still cheaper than a divorce  ht...\n",
       "28      Her: What you go do when yuh get this “Ching C...\n",
       "29                     @xenixfejzaj Men are the plague 👍🏼\n",
       "                              ...                        \n",
       "1970    @megatheorium Luckily, while I'm new at this i...\n",
       "1971    That may be true, but, that's also how the bla...\n",
       "1972               @slucciii Apparently animecanon=filler\n",
       "1973    How sad, this evil plague of propaganda loosed...\n",
       "1974    @SThepowerful @DoubleDogg3 It's not a simple q...\n",
       "1975    @Reuters Of coarse nothing to be worried about...\n",
       "1976    @moorviewgm @TracyBoyce16 @TallJosephine When ...\n",
       "1977    “you are a plague upon my life, kishimoto.”\\n“...\n",
       "1978    @PodSaveAmerica @jonfavs @jonlovett @TVietor08...\n",
       "1979    so this time i assume philza minecraft Will be...\n",
       "1980    @joshuajjb2 Boggles the mind that halfway dece...\n",
       "1981    @CaptPeteRider Don't be ridiculous.\\n\\nYou are...\n",
       "1982    @og_hapsburg @faciliheir @jeanlucpeakhard Avg ...\n",
       "1983    @Hugh_Henne Don’t take the vaccine bro! The ri...\n",
       "1984       @Bleach_plague Yeah lol. It needs to come back\n",
       "1985    @JenUwin2021 @Chris4Perkins It is the plague v...\n",
       "1986    @Kellen_the_man Joanna got a bunch of shit for...\n",
       "1987    @Dauhshanti @lotophagus I think the point @Dha...\n",
       "1988    @Krishna51886079 @DiamondandSilk They shouldn’...\n",
       "1989    @LiyaYuBerlin @SteffenWurzel @DavidJRMissal Is...\n",
       "1990    \"The invention of CRISPR and the plague of COV...\n",
       "1991    @themotleyfool #stocks $ACRX $SNDL https://t.c...\n",
       "1992    @continu_um @MattMcL_FTKE @goodearth_Helen @Hi...\n",
       "1993           I prefer unvaulted https://t.co/9jqGayooZd\n",
       "1994               @SsbMike Apparently filler=anime canon\n",
       "1995    @SilvioMedeiros I have only now found out abou...\n",
       "1996                  @BoredInterista The man is a plague\n",
       "1997    // TW transphobic slurs \\n\\nhe definitely knew...\n",
       "1998    \"WARNING: Scams are on the rise. An invaluable...\n",
       "1999    @piersmorgan Low life! She ghosted you?  This ...\n",
       "Name: text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
